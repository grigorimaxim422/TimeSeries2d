{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb35af6-c66f-4cd1-83f0-1eb06f408106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-01-13 21:07:00,099] A new study created in memory with name: no-name-10a6bbff-abc7-4bc1-a0fd-ca1c271fc8b1\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4184\\2999725087.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
      "[I 2025-01-13 21:08:09,951] Trial 0 finished with value: 0.00038584197252269155 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.47543014519211935, 'learning_rate': 0.0004550604683151981}. Best is trial 0 with value: 0.00038584197252269155.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4184\\2999725087.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
      "[I 2025-01-13 21:09:07,449] Trial 1 finished with value: 0.0003534032472419891 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.3173461625807401, 'learning_rate': 0.00030857233593809647}. Best is trial 1 with value: 0.0003534032472419891.\n",
      "[I 2025-01-13 21:10:19,148] Trial 2 finished with value: 0.0003907226002245972 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.2675322426388319, 'learning_rate': 0.0033139321570232563}. Best is trial 1 with value: 0.0003534032472419891.\n",
      "[I 2025-01-13 21:11:20,705] Trial 3 finished with value: 0.0005432587005832994 and parameters: {'hidden_dim': 128, 'num_layers': 4, 'dropout': 0.2963182930294327, 'learning_rate': 0.0004642580771967755}. Best is trial 1 with value: 0.0003534032472419891.\n",
      "[I 2025-01-13 21:12:14,581] Trial 4 finished with value: 0.00026803007726398806 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.46397735004538854, 'learning_rate': 0.0032041492063480904}. Best is trial 4 with value: 0.00026803007726398806.\n",
      "[I 2025-01-13 21:13:23,759] Trial 5 finished with value: 0.0002342943480471149 and parameters: {'hidden_dim': 160, 'num_layers': 4, 'dropout': 0.34529046261561647, 'learning_rate': 0.007958831985578183}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:14:24,027] Trial 6 finished with value: 0.006583865848369896 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.4068692062582885, 'learning_rate': 0.00873590147162997}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:14:46,099] Trial 7 finished with value: 0.00041382480677301913 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout': 0.39197416833242127, 'learning_rate': 0.0007777661213678592}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.435627758376697 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:14:56,605] Trial 8 finished with value: 0.0004395503148605878 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.435627758376697, 'learning_rate': 0.0030347905258815014}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:15:33,673] Trial 9 finished with value: 0.0011490220230453733 and parameters: {'hidden_dim': 32, 'num_layers': 4, 'dropout': 0.29175903332466424, 'learning_rate': 0.00014346222529399913}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:16:36,580] Trial 10 finished with value: 0.004528529890442521 and parameters: {'hidden_dim': 128, 'num_layers': 4, 'dropout': 0.10591223987227436, 'learning_rate': 0.00995643043442528}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4874893995213948 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:16:53,363] Trial 11 finished with value: 0.00029960845528297466 and parameters: {'hidden_dim': 160, 'num_layers': 1, 'dropout': 0.4874893995213948, 'learning_rate': 0.0025458247336437867}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:17:47,256] Trial 12 finished with value: 0.0005644542490534315 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.19979104352540003, 'learning_rate': 0.004912529189424636}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:18:13,837] Trial 13 finished with value: 0.0002459759509127418 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.36244465155728534, 'learning_rate': 0.0014299043952839127}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "[I 2025-01-13 21:18:53,618] Trial 14 finished with value: 0.0005498918379254809 and parameters: {'hidden_dim': 96, 'num_layers': 3, 'dropout': 0.36370360419017855, 'learning_rate': 0.0010776886085656727}. Best is trial 5 with value: 0.0002342943480471149.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3502137029007478 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:19:06,637] Trial 15 finished with value: 0.0001913842305922034 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.3502137029007478, 'learning_rate': 0.0011253698473746145}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21740895382206166 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:19:23,791] Trial 16 finished with value: 0.0006199377596865154 and parameters: {'hidden_dim': 160, 'num_layers': 1, 'dropout': 0.21740895382206166, 'learning_rate': 0.000129084508055818}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:20:16,124] Trial 17 finished with value: 0.00032256545752964235 and parameters: {'hidden_dim': 96, 'num_layers': 4, 'dropout': 0.3441079000126939, 'learning_rate': 0.0015221259103589911}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2440903726663422 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:20:34,674] Trial 18 finished with value: 0.0003980736430755562 and parameters: {'hidden_dim': 192, 'num_layers': 1, 'dropout': 0.2440903726663422, 'learning_rate': 0.0002244279775855246}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:21:08,099] Trial 19 finished with value: 0.0003147387840065428 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'dropout': 0.4114480269436158, 'learning_rate': 0.005989865101354294}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:22:09,611] Trial 20 finished with value: 0.0005656615407629447 and parameters: {'hidden_dim': 128, 'num_layers': 4, 'dropout': 0.14522790339296446, 'learning_rate': 0.000656423665372103}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:22:36,637] Trial 21 finished with value: 0.0005313510912608101 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.3545961660810371, 'learning_rate': 0.0012618287679102473}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3354939549775417 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:22:47,887] Trial 22 finished with value: 0.0005078891144049438 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.3354939549775417, 'learning_rate': 0.0017886164403531247}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:23:15,221] Trial 23 finished with value: 0.0007677693158091807 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.38513645557119497, 'learning_rate': 0.00211493564537159}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "C:\\Users\\User\\anaconda3\\envs\\RL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3137696379742136 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-01-13 21:23:32,031] Trial 24 finished with value: 0.00037902728548612106 and parameters: {'hidden_dim': 160, 'num_layers': 1, 'dropout': 0.3137696379742136, 'learning_rate': 0.0007467040674931665}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:24:02,998] Trial 25 finished with value: 0.00024376736175988546 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.43695482770862415, 'learning_rate': 0.004528152568736301}. Best is trial 15 with value: 0.0001913842305922034.\n",
      "[I 2025-01-13 21:25:03,332] Trial 26 finished with value: 0.00018315958037750204 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.43126904754814765, 'learning_rate': 0.005514447686616156}. Best is trial 26 with value: 0.00018315958037750204.\n",
      "[I 2025-01-13 21:26:03,812] Trial 27 finished with value: 0.00018045027371210216 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.449428080186037, 'learning_rate': 0.006742012077968675}. Best is trial 27 with value: 0.00018045027371210216.\n",
      "[I 2025-01-13 21:27:04,477] Trial 28 finished with value: 0.0003954584469531917 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.45066048468737485, 'learning_rate': 0.005898977153114189}. Best is trial 27 with value: 0.00018045027371210216.\n",
      "[I 2025-01-13 21:28:16,629] Trial 29 finished with value: 0.0004193705866451968 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.479110660462984, 'learning_rate': 0.0038470404496305484}. Best is trial 27 with value: 0.00018045027371210216.\n",
      "[I 2025-01-13 21:29:17,360] Trial 30 finished with value: 0.00019494243976871738 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.49400119713385837, 'learning_rate': 0.00723841019536123}. Best is trial 27 with value: 0.00018045027371210216.\n",
      "[I 2025-01-13 21:30:17,281] Trial 31 finished with value: 0.0007608636435840956 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.4995156134712436, 'learning_rate': 0.006650926864410055}. Best is trial 27 with value: 0.00018045027371210216.\n",
      "[I 2025-01-13 21:31:28,934] Trial 32 finished with value: 0.00012810321508864448 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.4332642026284568, 'learning_rate': 0.0048610015241686315}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:32:40,181] Trial 33 finished with value: 0.00047058888462329793 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.42178083293910956, 'learning_rate': 0.004727577448320377}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:33:51,906] Trial 34 finished with value: 0.00020357315621698615 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.4645984340176022, 'learning_rate': 0.002417513083105675}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:35:16,024] Trial 35 finished with value: 0.0007025191404285248 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout': 0.37665645068434606, 'learning_rate': 0.0003437024943482144}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:36:30,126] Trial 36 finished with value: 0.00021547637350687927 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.44906411828606285, 'learning_rate': 0.003842951170972452}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:37:45,745] Trial 37 finished with value: 0.0011571435316000134 and parameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.4008945660883368, 'learning_rate': 0.0005430131632783697}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:38:43,874] Trial 38 finished with value: 0.004448893048737029 and parameters: {'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.4277619254199991, 'learning_rate': 0.00821708852383711}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:39:39,218] Trial 39 finished with value: 0.0004308924504915591 and parameters: {'hidden_dim': 160, 'num_layers': 3, 'dropout': 0.2788741206454535, 'learning_rate': 0.0029658949240992634}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:41:09,535] Trial 40 finished with value: 0.0002692197310600684 and parameters: {'hidden_dim': 192, 'num_layers': 4, 'dropout': 0.4544388762675353, 'learning_rate': 0.003716081986655498}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:42:13,496] Trial 41 finished with value: 0.0002311276462437077 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.4765132951512403, 'learning_rate': 0.007283681642094238}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:43:19,261] Trial 42 finished with value: 0.005877180511809208 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.48812222868979316, 'learning_rate': 0.009838513832594072}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:44:16,562] Trial 43 finished with value: 0.0004100796438499608 and parameters: {'hidden_dim': 160, 'num_layers': 3, 'dropout': 0.40522803701679966, 'learning_rate': 0.005519248713189412}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:45:24,195] Trial 44 finished with value: 0.00015232925215968862 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.4331832420206925, 'learning_rate': 0.0073044053869952595}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:46:16,213] Trial 45 finished with value: 0.0002283715571552007 and parameters: {'hidden_dim': 224, 'num_layers': 2, 'dropout': 0.42865598753533374, 'learning_rate': 0.0044545674838136935}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:47:39,046] Trial 46 finished with value: 0.0002474282909216444 and parameters: {'hidden_dim': 192, 'num_layers': 4, 'dropout': 0.3762214024672502, 'learning_rate': 0.002859446625071772}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:49:26,485] Trial 47 finished with value: 0.002194728419205851 and parameters: {'hidden_dim': 224, 'num_layers': 4, 'dropout': 0.3298072305346923, 'learning_rate': 0.008522149021326532}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:50:58,664] Trial 48 finished with value: 0.0004972182240718129 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout': 0.46662239569782277, 'learning_rate': 0.00019695031115752546}. Best is trial 32 with value: 0.00012810321508864448.\n",
      "[I 2025-01-13 21:51:29,924] Trial 49 finished with value: 0.0003152960059444674 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.39554673407963176, 'learning_rate': 0.005468112668016296}. Best is trial 32 with value: 0.00012810321508864448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 224, 'num_layers': 3, 'dropout': 0.4332642026284568, 'learning_rate': 0.0048610015241686315}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"Book1.csv\")\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "data = data.sort_values(by=\"Date\")\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "numeric_data = data.drop(columns=[\"Date\"])\n",
    "scaled_data = scaler.fit_transform(numeric_data)\n",
    "data[numeric_data.columns] = scaled_data\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_data = data[(data[\"Date\"].dt.year >= 2017) & (data[\"Date\"].dt.year <= 2022)].drop(columns=[\"Date\"]).values\n",
    "val_data = data[data[\"Date\"].dt.year == 2023].drop(columns=[\"Date\"]).values\n",
    "test_data = data[data[\"Date\"].dt.year == 2024].drop(columns=[\"Date\"]).values\n",
    "\n",
    "# Convert data to sequences for time series\n",
    "def create_sequences(data, sequence_length=30):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(data[i + sequence_length, 0])  # Assuming target is the first column\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 30\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "X_val, y_val = create_sequences(val_data, sequence_length)\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# Prepare data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                        torch.tensor(y_train, dtype=torch.float32)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                      torch.tensor(y_val, dtype=torch.float32)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Define the LSTM with Attention model\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Output shape: (batch_size, seq_length, hidden_dim)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)  # Shape: (batch_size, seq_length, 1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)  # Weighted sum: (batch_size, hidden_dim)\n",
    "        output = self.fc(context_vector)  # Shape: (batch_size, 1)\n",
    "        return output\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTMWithAttention(X_train.shape[2], hidden_dim, num_layers, dropout)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    val_loss = evaluate_model(model, val_loader, device)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db834c-5578-4821-8b65-d5ee1bd36ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
